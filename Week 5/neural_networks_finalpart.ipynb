{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is Regularization?**  \n",
    "Regularization discourages overly large weights and improves generalization.\n",
    "\n",
    "Total Regularization Cost:\n",
    "- $ L_\\text{reg} = \\lambda_1 \\sum |w| + \\lambda_2 \\sum w^2 + \\dots $  \n",
    "Where:\n",
    "- $ \\lambda_1 $ = L1 regularization coefficient\n",
    "- $ \\lambda_2 $ = L2 regularization coefficient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Regularization loss\n",
    "class Loss:\n",
    "    def regularization_loss(self, layer):\n",
    "        regularization_loss = 0\n",
    "        if layer.weight_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
    "        if layer.weight_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
    "        if layer.bias_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
    "        if layer.bias_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\n",
    "\n",
    "        return regularization_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorical Cross-Entropy (CCE):**  \n",
    "Loss for multi-class classification:\n",
    "\n",
    "For sample $i$: $ L_i = -\\log p(y_i) $\n",
    "\n",
    "$ p(y_i) $ = predicted probability for the correct class.\n",
    "\n",
    "If labels are one-hot:\n",
    "$ p(y_i) = \\sum_j (y_\\text{true}[j] \\cdot y_\\text{pred}[j]) $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Categorical Cross-Entropy\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "\n",
    "        return -np.log(correct_confidences)\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        self.dinputs = self.dinputs / samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binary Cross-Entropy (BCE):**  \n",
    "Loss for binary classification:\n",
    "\n",
    "$ L_i = -[y_i \\log p_i + (1-y_i) \\log (1-p_i)] $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Binary Cross-Entropy\n",
    "class Loss_BinaryCrossentropy(Loss): \n",
    "    def forward(self, y_pred, y_true): \n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7) \n",
    "        sample_losses = -(y_true * np.log(y_pred_clipped) + (1-y_true) * np.log(1-y_pred_clipped))\n",
    "        sample_losses = np.mean(sample_losses, axis=-1) \n",
    "        return sample_losses\n",
    "\n",
    "    def backward(self, dvalues, y_true): \n",
    "        samples = len(dvalues) \n",
    "        outputs = len(dvalues[0]) \n",
    "        clipped_dvalues = np.clip(dvalues, 1e-7, 1-1e-7)\n",
    "\n",
    "        self.dinputs = -(y_true / clipped_dvalues - (1-y_true) / (1-clipped_dvalues))\n",
    "        self.dinputs = self.dinputs / outputs\n",
    "        self.dinputs = self.dinputs / samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Loop Concept:**  \n",
    "- Forward Pass:\n",
    "    - $ X \\to \\text{dense1.forward()} \\to \\text{activation1.forward()} $\n",
    "    - $ \\to \\dots \\to \\text{loss.forward()} $\n",
    "- Backward Pass:\n",
    "    - Gradients move backward using the chain rule:\n",
    "    $ \\nabla_\\theta L = \\nabla_\\theta L_\\text{out} \\cdot \\dots \\cdot \\nabla_\\theta L_\\text{in} $\n",
    "- Optimization:\n",
    "    - Parameters updated using the chosen optimization technique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Final Training Loop\n",
    "for epoch in range(10001):\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    dense2.forward(activation1.output)\n",
    "    activation2.forward(dense2.output)\n",
    "\n",
    "    data_loss = loss_function.calculate(activation2.output, y)\n",
    "    regularization_loss = loss_function.regularization_loss(dense1) + loss_function.regularization_loss(dense2)\n",
    "\n",
    "    loss = data_loss + regularization_loss\n",
    "    predictions = (activation2.output > 0.5) * 1\n",
    "    accuracy = np.mean(predictions == y)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'epoch: {epoch}, acc: {accuracy:.3f}, loss: {loss:.3f} (data_loss: {data_loss:.3f}, reg_loss: {regularization_loss:.3f}), lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    loss_function.backward(activation2.output, y)\n",
    "    activation2.backward(loss_function.dinputs)\n",
    "    dense2.backward(activation2.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validation:**  \n",
    "Evaluate the trained model on a separate test set:\n",
    "\n",
    "- Forward Pass:\n",
    "    $ X_\\text{test} \\to \\dots \\to \\text{activation2.output} $\n",
    "- Compare predicted labels vs actual labels\n",
    "- Output the final accuracy and loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Final Validation\n",
    "dense1.forward(X_test)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "loss = loss_function.calculate(activation2.output, y_test)\n",
    "predictions = (activation2.output > 0.5) * 1\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "\n",
    "print(f'Validation, acc: {accuracy:.3f}, loss: {loss:.3f}')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
